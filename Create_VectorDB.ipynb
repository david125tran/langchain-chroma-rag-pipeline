{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup (Install, mount drive, pull code)"
      ],
      "metadata": {
        "id": "trSehOyaPih1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Framework flags: tell Transformers to avoid TensorFlow/Flax ---\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# --- Install deps (pin requests so Drive auth stays happy) ---\n",
        "!pip install -U \"requests==2.32.4\" datasets langchain langchain-community langchain-chroma chromadb sentence-transformers tqdm langchain-huggingface\n",
        "\n",
        "# --- Hard restart the runtime so the env flags are in effect for all later imports ---\n",
        "import os, sys\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "PHpI3LG4Pq8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "qkdjnf2TaV24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify CUDA is usable\n",
        "import torch, requests\n",
        "print(\"requests:\", requests.__version__)\n",
        "print(\"torch:\", torch.__version__, \"cuda_available:\", torch.cuda.is_available())\n",
        "!nvidia-smi -L || true\n",
        "!nvidia-smi || true"
      ],
      "metadata": {
        "id": "63D4qFTqaXlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows a python process + rising memory while embeddings are computed\n",
        "!nvidia-smi -l 5\n"
      ],
      "metadata": {
        "id": "DG_RgWs-ajvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Script"
      ],
      "metadata": {
        "id": "Mqk88E5bPu4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, threading, subprocess\n",
        "from typing import List\n",
        "from datasets import load_dataset\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from tqdm import tqdm\n",
        "import chromadb  # for PersistentClient\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "PERSIST_DIR    = \"/content/drive/MyDrive/colab_chroma_10k\"   # lives in Drive\n",
        "COLLECTION     = \"edgar_10k\"\n",
        "INDEX_SECTIONS = {\"item_1\", \"item_1a\", \"item_7\"}       # start with key sections\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
        "\n",
        "TOTAL_ROWS = 6282   # train split size; used for ETA projection\n",
        "\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------- Embeddings on GPU (PyTorch) ----------------\n",
        "emb = HuggingFaceEmbeddings(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"batch_size\": 512, \"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# ---------------- Vector store (auto-persist via PersistentClient) ----------------\n",
        "client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
        "vectordb = Chroma(collection_name=COLLECTION, embedding_function=emb, client=client)\n",
        "\n",
        "# ---------------- Helpers ----------------\n",
        "def row_to_docs(row) -> List[Document]:\n",
        "    cik = str(row[\"cik\"])\n",
        "    date_str = str(row[\"date\"])[:10]\n",
        "    year = int(date_str[:4])\n",
        "    doc_id = f\"{cik}-{date_str}\"\n",
        "    docs: List[Document] = []\n",
        "    for k, v in row.items():\n",
        "        if not isinstance(v, str):\n",
        "            continue\n",
        "        k_lower = k.lower()\n",
        "        if not (k_lower.startswith(\"item_\") and v.strip()):\n",
        "            continue\n",
        "        if INDEX_SECTIONS and k_lower not in INDEX_SECTIONS:\n",
        "            continue\n",
        "        for idx, chunk in enumerate(splitter.split_text(v)):\n",
        "            docs.append(Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"doc_id\": doc_id, \"cik\": cik, \"company\": row[\"company\"],\n",
        "                    \"date\": date_str, \"year\": year, \"item\": k_lower,\n",
        "                    \"chunk_idx\": idx, \"row_idx\": int(row.get(\"__index_level_0__\", -1)),\n",
        "                }\n",
        "            ))\n",
        "    return docs\n",
        "\n",
        "def doc_id_for(d: Document):\n",
        "    m = d.metadata\n",
        "    return f\"{m['doc_id']}|{m['item']}|{m['chunk_idx']}\"\n",
        "\n",
        "# ---------------- Heartbeat + Rate/ETA ----------------\n",
        "print(\"✅ Starting streaming index…\", flush=True)\n",
        "\n",
        "stop_heartbeat = False\n",
        "progress = {\"rows\": 0, \"docs\": 0, \"last_batch\": 0}\n",
        "start = time.time()\n",
        "ema_rate = None\n",
        "ALPHA = 0.25\n",
        "\n",
        "def gpu_stats():\n",
        "    try:\n",
        "        out = subprocess.check_output(\n",
        "            [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used\", \"--format=csv,noheader,nounits\"]\n",
        "        ).decode().strip()\n",
        "        util, mem = out.split(',')\n",
        "        return int(util), int(mem)   # percent, MiB\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def rate_eta():\n",
        "    global ema_rate\n",
        "    elapsed = max(time.time() - start, 1e-6)\n",
        "    inst_rate = progress[\"docs\"] / elapsed\n",
        "    ema_rate = inst_rate if ema_rate is None else (ALPHA*inst_rate + (1-ALPHA)*ema_rate)\n",
        "    est_total = progress[\"docs\"] if progress[\"rows\"] == 0 else int((progress[\"docs\"]/progress[\"rows\"]) * TOTAL_ROWS)\n",
        "    remaining = max(est_total - progress[\"docs\"], 0)\n",
        "    eta_sec = remaining / ema_rate if ema_rate and ema_rate > 0 else float(\"inf\")\n",
        "    return ema_rate, eta_sec, est_total\n",
        "\n",
        "def heartbeat():\n",
        "    while not stop_heartbeat:\n",
        "        util, mem = gpu_stats()\n",
        "        r, eta, est_total = rate_eta()\n",
        "        eta_min = \"?\" if eta == float(\"inf\") else f\"{eta/60:.1f}m\"\n",
        "        try:\n",
        "            size = subprocess.check_output([\"du\",\"-sh\",PERSIST_DIR]).decode().split()[0]\n",
        "        except Exception:\n",
        "            size = \"?\"\n",
        "        print(\n",
        "            f\"[{time.strftime('%H:%M:%S')}] rows={progress['rows']} docs={progress['docs']} \"\n",
        "            f\"last={progress['last_batch']} db={size} \"\n",
        "            f\"gpu={util if util is not None else '-'}% mem={mem if mem is not None else '-'}MiB \"\n",
        "            f\"rate={0 if r is None else r:.1f}/s ETA~{eta_min} (est_total≈{est_total:,})\",\n",
        "            flush=True\n",
        "        )\n",
        "        time.sleep(60)\n",
        "\n",
        "threading.Thread(target=heartbeat, daemon=True).start()\n",
        "\n",
        "# ---------------------------------------- (1) Knowledge Base ----------------------------------------\n",
        "KNOWLEDGE_BASE = \"jlohding/sp500-edgar-10k\"\n",
        "\n",
        "# ---------------------------------------- (2) Load ----------------------------------------\n",
        "dataset = load_dataset(KNOWLEDGE_BASE, split=\"train\", streaming=True)\n",
        "\n",
        "# ---------------------------------------- (3) Preprocess & Chunk ----------------------------------------\n",
        "BATCH = 500\n",
        "LOG_EVERY = 10  # formerly PERSIST_EVERY\n",
        "since_log = 0\n",
        "buf_docs, buf_ids = [], []\n",
        "try:\n",
        "    with tqdm(unit=\"vec\", mininterval=2, desc=\"Indexing\") as pbar:\n",
        "        for row in dataset:\n",
        "            progress[\"rows\"] += 1\n",
        "\n",
        "            # ---------------------------------------- (4) Embed & Index in Vector Store ----------------------------------------\n",
        "            for d in row_to_docs(row):\n",
        "                buf_docs.append(d); buf_ids.append(doc_id_for(d))\n",
        "                if len(buf_docs) >= BATCH:\n",
        "                    vectordb.add_documents(buf_docs, ids=buf_ids)  # writes to disk\n",
        "                    progress[\"docs\"] += len(buf_docs)\n",
        "                    progress[\"last_batch\"] = len(buf_docs)\n",
        "                    pbar.update(len(buf_docs))\n",
        "                    buf_docs.clear(); buf_ids.clear()\n",
        "except KeyboardInterrupt:\n",
        "    if buf_docs:\n",
        "        vectordb.add_documents(buf_docs, ids=buf_ids)  # flush remainder\n",
        "        progress[\"docs\"] += len(buf_docs)\n",
        "        print(f\"Interrupted — flushed {len(buf_docs)} docs.\", flush=True)\n",
        "finally:\n",
        "    stop_heartbeat = True\n",
        "r, eta, est = rate_eta()\n",
        "print(f\"✅ Done. Chroma DB at: {PERSIST_DIR}\")\n",
        "print(f\"Final rate≈{r:.1f} docs/s • total_docs={progress['docs']:,}\")\n"
      ],
      "metadata": {
        "id": "isMF4MdxP3Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, subprocess, shutil, chromadb\n",
        "\n",
        "SRC = \"/content/drive/MyDrive/colab_chroma_10k\"\n",
        "STAMP = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "SNAP = f\"/content/drive/MyDrive/colab_chroma_10k_snap_{STAMP}\"\n",
        "\n",
        "# 1) Touch the DB once to let SQLite clean any journal\n",
        "client = chromadb.PersistentClient(path=SRC)\n",
        "print(\"Collections:\", [c.name for c in client.list_collections()])\n",
        "del client  # close handle\n",
        "\n",
        "# 2) Snapshot the folder (so you zip a stable copy)\n",
        "os.makedirs(SNAP, exist_ok=True)\n",
        "subprocess.run([\"rsync\",\"-a\",\"--delete\", f\"{SRC}/\", f\"{SNAP}/\"], check=True)\n",
        "print(\"Snapshot:\", SNAP)\n"
      ],
      "metadata": {
        "id": "w1Npqh-y7Eg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "ZIP_BASE = f\"/content/drive/MyDrive/edgar_10k_chroma_{STAMP}\"\n",
        "shutil.make_archive(ZIP_BASE, \"zip\", SNAP)\n",
        "print(\"Created:\", ZIP_BASE + \".zip\")\n"
      ],
      "metadata": {
        "id": "dpmVRgssVaS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(ZIP_BASE + \".zip\")\n"
      ],
      "metadata": {
        "id": "rqhc3BJ2Vnqi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}